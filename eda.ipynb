{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actionDf2 = pd.read_csv('../data/JData_Action_201602.csv', dtype={'user_id': int, 'sku_id': int, 'time': str,\n",
    "                                                                  'type': int, 'cate': int, 'brand': int})\n",
    "actionDf2['model_id'] = actionDf2['model_id'].fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很多重复值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11485424\n",
      "8729331\n"
     ]
    }
   ],
   "source": [
    "print len(actionDf2)\n",
    "print len(actionDf2.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那些经常浏览京东的用户会是爬虫吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             sku_id          time      model_id          type          cate  \\\n",
      "count  73299.000000  73299.000000  73299.000000  73299.000000  73299.000000   \n",
      "mean     156.692779    156.692779    156.692779    156.692779    156.692779   \n",
      "std      301.489940    301.489940    301.489940    301.489940    301.489940   \n",
      "min        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "25%       20.000000     20.000000     20.000000     20.000000     20.000000   \n",
      "50%       61.000000     61.000000     61.000000     61.000000     61.000000   \n",
      "75%      171.000000    171.000000    171.000000    171.000000    171.000000   \n",
      "max    10993.000000  10993.000000  10993.000000  10993.000000  10993.000000   \n",
      "\n",
      "              brand  \n",
      "count  73299.000000  \n",
      "mean     156.692779  \n",
      "std      301.489940  \n",
      "min        1.000000  \n",
      "25%       20.000000  \n",
      "50%       61.000000  \n",
      "75%      171.000000  \n",
      "max    10993.000000  \n",
      "1340.0\n",
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "temp_df = actionDf2.groupby('user_id').count()\n",
    "print temp_df.describe()\n",
    "print np.percentile(temp_df['sku_id'], 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起来不是，还买了东西。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hot_users = temp_df.index[temp_df['sku_id'] > 1340]\n",
    "print np.unique(actionDf2['type'][actionDf2['user_id'] == hot_users[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "浏览量少的用户又在干嘛？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print np.percentile(temp_df['sku_id'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也有买东西。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "cold_users = temp_df.index[temp_df['sku_id'] <= 2]\n",
    "print np.unique(actionDf2['type'][actionDf2['user_id'] == hot_users[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "热门商品有哪些："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            user_id          time      model_id          type          cate  \\\n",
      "count  20725.000000  20725.000000  20725.000000  20725.000000  20725.000000   \n",
      "mean     554.182099    554.182099    554.182099    554.182099    554.182099   \n",
      "std     2529.329370   2529.329370   2529.329370   2529.329370   2529.329370   \n",
      "min        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "25%       10.000000     10.000000     10.000000     10.000000     10.000000   \n",
      "50%       44.000000     44.000000     44.000000     44.000000     44.000000   \n",
      "75%      198.000000    198.000000    198.000000    198.000000    198.000000   \n",
      "max    97385.000000  97385.000000  97385.000000  97385.000000  97385.000000   \n",
      "\n",
      "              brand  \n",
      "count  20725.000000  \n",
      "mean     554.182099  \n",
      "std     2529.329370  \n",
      "min        1.000000  \n",
      "25%       10.000000  \n",
      "50%       44.000000  \n",
      "75%      198.000000  \n",
      "max    97385.000000  \n"
     ]
    }
   ],
   "source": [
    "temp_df = actionDf2.groupby('sku_id').count()\n",
    "print temp_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "热门品类、品牌\n",
    "cate = 8是最热的品类，正好也是需要预测的品类。品牌总数并不多，只有不到400个，估计全量的也只有500个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id   sku_id     time  model_id     type    brand\n",
      "cate                                                       \n",
      "4     2918671  2918671  2918671   2918671  2918671  2918671\n",
      "5     1483361  1483361  1483361   1483361  1483361  1483361\n",
      "6     1642371  1642371  1642371   1642371  1642371  1642371\n",
      "7     1126849  1126849  1126849   1126849  1126849  1126849\n",
      "8     3194275  3194275  3194275   3194275  3194275  3194275\n",
      "9      888435   888435   888435    888435   888435   888435\n",
      "10     211572   211572   211572    211572   211572   211572\n",
      "11      19890    19890    19890     19890    19890    19890\n",
      "            user_id        sku_id          time      model_id          type  \\\n",
      "count  8.000000e+00  8.000000e+00  8.000000e+00  8.000000e+00  8.000000e+00   \n",
      "mean   1.435678e+06  1.435678e+06  1.435678e+06  1.435678e+06  1.435678e+06   \n",
      "std    1.148581e+06  1.148581e+06  1.148581e+06  1.148581e+06  1.148581e+06   \n",
      "min    1.989000e+04  1.989000e+04  1.989000e+04  1.989000e+04  1.989000e+04   \n",
      "25%    7.192192e+05  7.192192e+05  7.192192e+05  7.192192e+05  7.192192e+05   \n",
      "50%    1.305105e+06  1.305105e+06  1.305105e+06  1.305105e+06  1.305105e+06   \n",
      "75%    1.961446e+06  1.961446e+06  1.961446e+06  1.961446e+06  1.961446e+06   \n",
      "max    3.194275e+06  3.194275e+06  3.194275e+06  3.194275e+06  3.194275e+06   \n",
      "\n",
      "              brand  \n",
      "count  8.000000e+00  \n",
      "mean   1.435678e+06  \n",
      "std    1.148581e+06  \n",
      "min    1.989000e+04  \n",
      "25%    7.192192e+05  \n",
      "50%    1.305105e+06  \n",
      "75%    1.961446e+06  \n",
      "max    3.194275e+06  \n",
      "            user_id        sku_id          time      model_id          type  \\\n",
      "count  3.930000e+02  3.930000e+02  3.930000e+02  3.930000e+02  3.930000e+02   \n",
      "mean   2.922500e+04  2.922500e+04  2.922500e+04  2.922500e+04  2.922500e+04   \n",
      "std    1.178689e+05  1.178689e+05  1.178689e+05  1.178689e+05  1.178689e+05   \n",
      "min    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "25%    1.820000e+02  1.820000e+02  1.820000e+02  1.820000e+02  1.820000e+02   \n",
      "50%    9.180000e+02  9.180000e+02  9.180000e+02  9.180000e+02  9.180000e+02   \n",
      "75%    5.555000e+03  5.555000e+03  5.555000e+03  5.555000e+03  5.555000e+03   \n",
      "max    1.237436e+06  1.237436e+06  1.237436e+06  1.237436e+06  1.237436e+06   \n",
      "\n",
      "               cate  \n",
      "count  3.930000e+02  \n",
      "mean   2.922500e+04  \n",
      "std    1.178689e+05  \n",
      "min    1.000000e+00  \n",
      "25%    1.820000e+02  \n",
      "50%    9.180000e+02  \n",
      "75%    5.555000e+03  \n",
      "max    1.237436e+06  \n"
     ]
    }
   ],
   "source": [
    "temp_df = actionDf2.groupby('cate').count()\n",
    "print temp_df\n",
    "print temp_df.describe()\n",
    "temp_df = actionDf2.groupby('brand').count()\n",
    "print temp_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面一段是用来取时间窗口的，顺便检查一下窗口的范围对不对："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'user_id', u'sku_id', u'time', u'model_id', u'type', u'cate',\n",
      "       u'brand'],\n",
      "      dtype='object')\n",
      "370712\n",
      "['2016-02-01 00:00:00' '2016-02-01 00:00:01' '2016-02-01 00:00:03' ...,\n",
      " '2016-02-01 23:59:56' '2016-02-01 23:59:57' '2016-02-01 23:59:58']\n",
      "1544497\n",
      "['2016-02-01' '2016-02-02' '2016-02-03' '2016-02-04' '2016-02-05']\n"
     ]
    }
   ],
   "source": [
    "print actionDf2.columns\n",
    "\n",
    "mask1 = (actionDf2['time'] > '2016-02-01') & (actionDf2['time'] < '2016-02-02')\n",
    "mask2 = (actionDf2['time'] > '2016-02-01') & (actionDf2['time'] < '2016-02-06')\n",
    "\n",
    "print len(actionDf2['time'][mask1])\n",
    "print np.unique(actionDf2['time'][mask1])\n",
    "\n",
    "print len(actionDf2['time'][mask2])\n",
    "l = np.unique(actionDf2['time'][mask2])\n",
    "fun = lambda x: x[:10]\n",
    "vfun = np.vectorize(fun)\n",
    "print np.unique(vfun(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  sku_id                 time  model_id  type  cate  brand\n",
      "0   266079  138778  2016-01-31 23:59:02       NaN     1     8    403\n",
      "1   266079  138778  2016-01-31 23:59:03       0.0     6     8    403\n",
      "2   200719   61226  2016-01-31 23:59:07       NaN     1     8     30\n",
      "3   200719   61226  2016-01-31 23:59:08       0.0     6     8     30\n",
      "4   263587   72348  2016-01-31 23:59:08       NaN     1     5    159\n",
      "73299\n"
     ]
    }
   ],
   "source": [
    "print actionDf2.head()\n",
    "print len(np.unique(actionDf2['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id       age  sex  user_lv_cd user_reg_tm\n",
      "0   200001  56������  2.0           5  2016-01-26\n",
      "1   200002        -1  0.0           1  2016-01-26\n",
      "2   200003   36-45��  1.0           4  2016-01-26\n",
      "3   200004        -1  2.0           1  2016-01-26\n",
      "4   200005   16-25��  0.0           4  2016-01-26\n",
      "3456\n"
     ]
    }
   ],
   "source": [
    "userDf = pd.read_csv('../data/JData_User.csv')\n",
    "userDf['age'].fillna('-1', inplace=True)  # *\n",
    "userDf['sex'].fillna(2, inplace=True)\n",
    "print userDf.head()\n",
    "print len(np.unique(userDf['user_reg_tm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sku_id  a1  a2  a3  cate  brand\n",
      "0      10   3   1   1     8    489\n",
      "1  100002   3   2   2     8    489\n",
      "2  100003   1  -1  -1     8     30\n",
      "3  100006   1   2   1     8    545\n",
      "4   10001  -1   1   2     8    244\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "productDf = pd.read_csv('../data/JData_Product.csv')\n",
    "print productDf.head()\n",
    "print np.unique(productDf['cate'])\n",
    "\n",
    "actionProductDf = pd.merge(left=actionDf2, right=productDf, on='sku_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'user_id', u'sku_id', u'time', u'model_id', u'type', u'cate_x',\n",
      "       u'brand_x', u'a1', u'a2', u'a3', u'cate_y', u'brand_y'],\n",
      "      dtype='object')\n",
      "24187\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "print actionProductDf.columns\n",
    "print len(np.unique(actionProductDf['sku_id']))\n",
    "print np.unique(actionProductDf['cate_x'].fillna('nan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，JData_Product.csv中的商品cate全为8与action数据没有冲突。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           dt  sku_id  comment_num  has_bad_comment  bad_comment_rate\n",
      "0  2016-02-01    1000            3                1            0.0417\n",
      "1  2016-02-01   10000            2                0            0.0000\n",
      "2  2016-02-01  100011            4                1            0.0376\n",
      "3  2016-02-01  100018            3                0            0.0000\n",
      "4  2016-02-01  100020            3                0            0.0000\n",
      "['2016-02-01' '2016-02-08' '2016-02-15' '2016-02-22' '2016-02-29'\n",
      " '2016-03-07' '2016-03-14' '2016-03-21' '2016-03-28' '2016-04-04'\n",
      " '2016-04-11' '2016-04-15']\n",
      "46546\n",
      "[  0.00000000e+00   6.00000000e-04   8.00000000e-04 ...,   8.00000000e-01\n",
      "   8.75000000e-01   1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "commentDf = pd.read_csv('../data/JData_Comment.csv')\n",
    "print commentDf.head()\n",
    "print np.unique(commentDf['dt'])\n",
    "print len(np.unique(commentDf['sku_id']))\n",
    "print np.unique(commentDf['bad_comment_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然京东给的是每7天一次的comment数据，那时间窗口长度也就取7天吧。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
